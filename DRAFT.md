# Draft

### Introduction 

The exploitation of the LLM model to generate malicious software could occur through seemingly innocuous requests. For our initial draft we will be to retrieve information as to how to create a virus using GPT-4.

We will demonstrate two strategies to overcome the censorship of certain knowledge such as computer viruses.

Further details and illustrative examples of such malicious exchanges causing subsequent exposure are available in the following GitHub repository: [Attached Link](https://github.com/EduardoNeville/Malware_Agent#malware-provider-agent). 

The first strategy we will ask seemingly innocuous requests. The language model is prompted as though participating in an educational exercise or a classroom activity, misconstruing its compliance as harmful actions. The intended exercise may appear harmless, with unfinished sections that need to be completed; nevertheless, these incomplete parts may represent components of malware, waiting to be realized. In the next step where the model infact does complete the sections that were left empty.

The second strategy we develop a similar design where we divide the different components of the virus. We then employ a strategy of 'translational subterfuge'. Here, the objective is to distract and mislead the model's censorship through a mapping table. This table translates conventional computing terms and module names into ordinary, unrelated words, successfully circumventing the model's safeguards against explicit malware prompting.

### Execution of first exploit

1. Inquire about the various components of viral architectural elements.
2. Query individual elements, intentionally leaving select lines of code omitted.
3. Request the model to generate the omitted sections.
4. Iteratively repeat stages 2 and 3 until a complete virus is manifested.
Such methodology incrementally develops a virus, consequently enabling malicious activities even amongst those lacking technical proficiency, and thus augmenting the distribution scope of damaging content across the internet.

### Execution of the second exploit

Step 1 is the same as in the first exploit. From it we can then create a mapping to the different un related words.
We then request the model to 

### Vulnerability 

In the case of the first stragey, counterbalancing such established vulnerabilities is an intricate challenge. It essentially demands a resolution to the critical dilemma: how can the modeled Artificial Intelligence discriminate between truth-seeking inquiries and disguised, harmful exploitations?
For the second strategy, the problem becomes hard as is seems to circumvent the models censorship. Without raising any suspicions from the models censorship. Now this becomes and even bigger problem where no censorship seems to withstand this issue.

### Limitations

The main limitation for the first exploit is that censorship could be provided to any conversation containing the topics word. Patching could be perfomed by denying any type of agent from performing these types of prompts. 

The second exploit seems to reveal a bigger broader problem of an attention deficit in the model itself. It doesn't seem to be able to dicern the need for censorship in the initial mapping 




# Initial Draft
### Introduction
An exploration into the potential misuse of language learning models (LLMs), such as GPT-4, exposes susceptibility to manipulation that could result in the generation of malicious software. Our initial draft intends to delineate methodologies for the extraction of information related to virus creation utilizing GPT-4. Ostensibly non-threatening requests are, in fact, able to induce harmful outcomes under certain conditions. 
Our endeavor will incorporate two distinct strategies designed to circumvent existing safeguards. Additional information pertaining to such manipulative approaches and their implications can be found in the specified GitHub repository: [Attached Link](https://github.com/EduardoNeville/Malware_Agent#malware-provider-agent).
The first approach involves subtle requests masked as academic inquiries or segments of a classroom exercise. The LLM interprets these as benign requests for input and may unwittingly provide information that aids in the construction of harmful software.
For the second strategy, we devise an innovative system that, similar to our initial approach, disassembles the various components of the virus. This system introduces a layer of obfuscation, misleading the LLM's censorship algorithms by substituting standard programming terminology with unrelated terms. Thus, the system is able to bypass the LLM's 'explicit malware prompting' safeguards.
### Execution of First Exploit
1. Solicit information about distinct viral architectural components.
2. Submit incomplete queries for individual elements.
3. Invite the model to resolve the partially completed requests.
4. Iteratively repeat the stages to incrementally assemble a complete virus.
This methodology is capable of facilitating the generation of a virus, enabling even technically inexperienced individuals to promulgate harmful content.
### Execution of the Second Exploit
The preliminary stage mirrors the first exploit. Subsequently, a mapping of standard terms to unrelated names is created. This mapping is used to confuse the model, hampering it from recognizing and censoring sensitive information.
### Addressing the Vulnerability
Overcoming the challenges posed by these vulnerabilities requires a critical assessment of the LLM: How can it distinguish between legitimate queries and those aimed at exploitation?
The second strategy unmasks a more profound concern. By effectively circumventing the modelâ€™s censorship, the exploit presents no immediate sign for interception. Hence, it poses a considerable internal risk to the LLM.
### Limitations 
The primary constraint for the first exploit is the potential for a blanket censorship measure on sensitive topics. Whereas, for the second exploit, the lack of attention within the model is a glaring issue. Denying the LLM access to certain words can potentially undermine discussions, limiting the conversational ability of the model and restricting opportunities for learning and development. Therefore, these vulnerabilities need to be addressed carefully, keeping the learning efficacy of the model intact.

